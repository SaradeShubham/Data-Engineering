{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JRyfUSdtZs3Z"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "conf=SparkConf().setAppName(\"sample\")\n",
        "\n",
        "sc=SparkContext(conf=conf)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list1=[1,2,3,4,5,6]\n",
        "\n",
        "print(type(list1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wML2PCexZz0q",
        "outputId": "2e747dda-8e6e-4aee-c771-6450875c302f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list1=[1,2,3,4,5,6]\n",
        "\n",
        "## to convrt data type like list,tuple we have to use function called Parallelize\n",
        "## Syntaxt: rdd=sc.parallelize(\" file \")\n",
        "\n",
        "rdd1=sc.parallelize(list1)\n",
        "\n",
        "## to view the result we have also use another function calles collect\n",
        "\n",
        "print(rdd1.collect())\n",
        "\n",
        "print(type (rdd1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uArTvAH1Zzxv",
        "outputId": "279c1d5b-35be-4725-d429-503a9950105b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 6]\n",
            "<class 'pyspark.core.rdd.RDD'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Task: now we have to get values from our RDD which <=4(Less than equal to 4)\n",
        "## Filter : it id to filter out our RDD\n",
        "##we did not able to perform a operation on RDD while using Filter it judt helpto filter out the elements.\n",
        "\n",
        "result_rdd=rdd1.filter(lambda x : x<=4)\n",
        "\n",
        "print(result_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoYz84BrZzu6",
        "outputId": "120c2be4-b03a-41f1-c9af-511b8a7ca3b5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Task: i have to add 5 in every element which i have in my RDD\n",
        "## Map Fuction: this function is able to perform operation on evry element of the RDD\n",
        "\n",
        "result_rdd=rdd1.map(lambda x : x+5)\n",
        "\n",
        "print(result_rdd.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eZWWRvcZzri",
        "outputId": "ec34190a-2583-4113-b26b-d9ba812febbe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6, 7, 8, 9, 10, 11]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list1=[\"hello students\", \"pyspark session is started\"]\n",
        "\n",
        "rdd_list1=sc.parallelize(list1)\n",
        "\n",
        "result_rdd=rdd_list1.map(lambda x : x.split(\" \"))\n",
        "\n",
        "print(result_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_lFnHhQZzlI",
        "outputId": "e1e1029e-5fcb-4a6d-c7a9-f1de72622591"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['hello', 'students'], ['pyspark', 'session', 'is', 'started']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## now by using map function we got to diff list for two string\n",
        "## but to get single flatten list we have to use flatMap function\n",
        "\n",
        "##flatMap function: this function flatten all data in a sinle list in our RDD\n",
        "\n",
        "result_rdd=rdd_list1.flatMap(lambda x : x.split(\" \"))\n",
        "\n",
        "print(result_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGRduwrcmmRw",
        "outputId": "d69b9750-9143-4fcc-8723-b241eb8bdd99"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'students', 'pyspark', 'session', 'is', 'started']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list1=[\"hello\", \"students\", \"pyspark\", \"session\", \"is\" \"started\"]\n",
        "\n",
        "rdd_list1=sc.parallelize(list1)\n",
        "\n",
        "result_rdd=rdd_list1.map(lambda x : x.split(\" \"))\n",
        "\n",
        "print(result_rdd.collect()) ## here we get every element separated but each string belong to diff list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEIESnKAmmMs",
        "outputId": "d476eae9-0c78-48a5-aa1a-f95e47d7f472"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['hello'], ['students'], ['pyspark'], ['session'], ['isstarted']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list1=[\"hello\", \"students\", \"pyspark\", \"session\", \"is\" \"started\"]\n",
        "\n",
        "rdd_list1=sc.parallelize(list1)\n",
        "\n",
        "result_rdd=rdd_list1.flatMap(lambda x : x.split(\" \"))\n",
        "\n",
        "print(result_rdd.collect()) ## here we get flatten data as a single list or RDD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FI0TbUaemmGw",
        "outputId": "a6b29e88-6a9a-4e28-95dc-a92b733f0d2c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'students', 'pyspark', 'session', 'isstarted']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(rdd_list1.collect())\n",
        "\n",
        "print(result_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8Y3jl_ppQ7x",
        "outputId": "6ab0af60-62f9-4576-e079-8486d5bb0c0f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello students', 'pyspark session is started']\n",
            "['hello', 'students', 'pyspark', 'session', 'is', 'started']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Count fuction: it use to count of how many element in our RDD\n",
        "\n",
        "print(result_rdd.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWxgNqtkpQ41",
        "outputId": "54cb341b-58ce-445e-ca1a-fad999076c24"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## len() function : we call length of each elemnt which we having in our RDD\n",
        "\n",
        "len_rdd=result_rdd.map(lambda x : len(x))\n",
        "\n",
        "print(len_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZtYMsCHpQ2P",
        "outputId": "dc607a0c-6f80-4033-a34a-295c71a71e4c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5, 8, 7, 7, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### now i want output in form of element,len()\n",
        "\n",
        "sep_rdd=result_rdd.map(lambda x : (x,len(x)))\n",
        "\n",
        "print(sep_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ya0KPBuypQzY",
        "outputId": "5825d290-b346-4bb0-d268-25a2dd84117c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('hello', 5), ('students', 8), ('pyspark', 7), ('session', 7), ('isstarted', 9)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list1=[\"hello students\", \"pyspark session is started\", \"students are doing pyspark scenarios\"]\n",
        "rdd_list=sc.parallelize(list1)\n",
        "\n",
        "result_rdd=rdd_list.flatMap(lambda x: x.split(\" \"))\n",
        "print(result_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n99GLPKBpQwh",
        "outputId": "358ca481-3f7c-432c-c7f7-3060d85da5fc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'students', 'pyspark', 'session', 'is', 'started', 'students', 'are', 'doing', 'pyspark', 'scenarios']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd2=result_rdd.map(lambda x: (x,1))\n",
        "\n",
        "print(rdd2.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGt7GQU4pQty",
        "outputId": "ab17d08d-8874-4448-ff83-514930429f4b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('hello', 1), ('students', 1), ('pyspark', 1), ('session', 1), ('is', 1), ('started', 1), ('students', 1), ('are', 1), ('doing', 1), ('pyspark', 1), ('scenarios', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_rdd2=rdd2.reduceByKey(lambda x,y : x+y)\n",
        "\n",
        "print(result_rdd2.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CoWU--HpQqs",
        "outputId": "1d5da78d-ad11-4876-e345-9e1e76ff6405"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('hello', 1), ('students', 2), ('session', 1), ('started', 1), ('are', 1), ('doing', 1), ('pyspark', 2), ('is', 1), ('scenarios', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IiGxJRXEmmEH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list=sc.parallelize([1,2,2,1,2,3,1,3,1,2,3,3,2,3,2,3,2,])\n",
        "### Distinct Function= here by using the distinct function we re able to remove duplicates from our RDD\n",
        "\n",
        "distinct_rdd=list.distinct()\n",
        "\n",
        "print(distinct_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyGOD_CjygCc",
        "outputId": "da6c4071-0468-4f5c-ad3a-d6d8cef49148"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 1, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### reading the txt file and create RDD of it to perform the task\n",
        "#Task :Read a text file amd make rdd and count the words.\n",
        "rdd_txt=sc.textFile(\"Hello Shubham.txt\")\n",
        "\n",
        "print(rdd_txt.collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOLaI1gmyf_x",
        "outputId": "afb2088a-aa8e-400f-9184-f427cc4330df"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello Shubham', 'hello Vishnu ', 'Hello Charan', 'Hello Bharath']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "task_rdd=rdd_txt.flatMap(lambda x : x.split(\" \"))\\\n",
        "                .map(lambda x : (x,1))\\\n",
        "                .reduceByKey (lambda x,y : x+y)\n",
        "\n",
        "print(task_rdd.collect())\n",
        "\n",
        "## Now we want to save this results\n",
        "## saveAsTextFile Function\n",
        "\n",
        "task_rdd.saveAsTextFile(\"Results_txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOue0jmQ2IyW",
        "outputId": "7df23359-efb5-407f-98ff-e03c2e488129"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Shubham', 1), ('hello', 1), ('', 1), ('Charan', 1), ('Bharath', 1), ('Hello', 3), ('Vishnu', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## take function: by using this function we can can mention how any elaements we have to show\n",
        "\n",
        "list=sc.parallelize([1,2,3,4,5,6,7,8,9])\n",
        "\n",
        "# here i want just first 5 elements\n",
        "\n",
        "first_5=list.take(5)\n",
        "print(first_5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAlLzent2Iv5",
        "outputId": "606ad00d-1399-42de-a68b-95e44c0390a6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Upload a csv file and read it and create a data frame\n",
        "csv_rdd=sc.textFile(\"dept.csv\")\n",
        "\n",
        "print(csv_rdd.collect())\n",
        "\n",
        "##now trying to create data frame of our rdd\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark= SparkSession.builder.appName(\"pract\").getOrCreate()\n",
        "\n",
        "df=spark.createDataFrame(csv_rdd)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "id": "CrM17mvC2ItU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "a8a9c856-26d8-43d4-ad83-a628dcc0c825"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['accountNumber,assetValue', 'DEPT3,500', 'DEPT3,300', 'DEPT1,1000', 'DEPT1,700', 'DEPT1,500', 'DEPT2,400', 'DEPT2,200']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PySparkTypeError",
          "evalue": "[CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `str`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-142659569.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pract\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_rdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1597\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m             )\n\u001b[0;32m-> 1599\u001b[0;31m         return self._create_dataframe(\n\u001b[0m\u001b[1;32m   1600\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverifySchema\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1639\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_remote_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1641\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1642\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1643\u001b[0m             rdd, struct = self._createFromLocal(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[0;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         \"\"\"\n\u001b[1;32m   1160\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m             \u001b[0mtupled_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[0;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0mprefer_timestamp_ntz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_timestamp_ntz_preferred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msamplingRatio\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m             schema = _infer_schema(\n\u001b[0m\u001b[1;32m   1111\u001b[0m                 \u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_infer_schema\u001b[0;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, infer_map_from_first_pair, prefer_timestamp_ntz)\u001b[0m\n\u001b[1;32m   2381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2382\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2383\u001b[0;31m         raise PySparkTypeError(\n\u001b[0m\u001b[1;32m   2384\u001b[0m             \u001b[0merrorClass\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"CANNOT_INFER_SCHEMA_FOR_TYPE\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2385\u001b[0m             \u001b[0mmessageParameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"data_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPySparkTypeError\u001b[0m: [CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `str`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RLmqLy3e2Iqd"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ViBV10jj2Inf"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oZp8cc_E2Ikf"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KQiBbWN62Ig_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ifWPlaPvyf6y"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eQirBJG0yf4U"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FYlt_8ZLyf1M"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YB-ILYqnyfx_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JxPDQY_iyfsz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eSlS7xOVmmBa"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7shf1y6Iml-g"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9njIUKuSml7h"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dLM0ILBPml4K"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kNU0hKWkZzgT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QhhCLDVDZzdp"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pkPG9PtIZzXx"
      },
      "execution_count": 25,
      "outputs": []
    }
  ]
}