{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "FI37mDOAS413"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "conf=SparkConf().setAppName(\"sample\")\n",
        "\n",
        "sc=SparkContext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "conf = SparkConf().setAppName(\"sample\")\n",
        "\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "\n",
        "\n",
        "spark= SparkSession.builder.appName(\"pract\").getOrCreate()\n",
        "\n",
        "\n",
        "df=spark.read.csv(\"dept.csv\", header=True, inferSchema=True)\n",
        "\n",
        "df.show()\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7AOtCSgVzCP",
        "outputId": "ad748536-faf0-4421-fac7-202dcaba8e38"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+\n",
            "|accountNumber|assetValue|\n",
            "+-------------+----------+\n",
            "|        DEPT3|       500|\n",
            "|        DEPT3|       300|\n",
            "|        DEPT1|      1000|\n",
            "|        DEPT1|       700|\n",
            "|        DEPT1|       500|\n",
            "|        DEPT2|       400|\n",
            "|        DEPT2|       200|\n",
            "+-------------+----------+\n",
            "\n",
            "root\n",
            " |-- accountNumber: string (nullable = true)\n",
            " |-- assetValue: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## now we have to tackel with the error regarding the infer schema\n"
      ],
      "metadata": {
        "id": "v9fyX8wuVy7K"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_rdd=sc.textFile(\"Hello Shubham.txt\")\n",
        "\n",
        "print(text_rdd.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vii3RKgFZlHp",
        "outputId": "ec1104a3-1d1e-4897-af0d-bccd601c7a3e"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello Shubham', 'hello Vishnu ', 'Hello Charan', 'Hello Bharath']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1=sc.parallelize([1,2,3,4,5,6])\n",
        "\n",
        "rdd2=sc.parallelize([3,4,5,6,7,8,9])\n",
        "\n",
        "## Union function==> it make one single rdd with contain all duplicates also\n",
        "\n",
        "rdd3=rdd1.union(rdd2)\n",
        "\n",
        "print(rdd3.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzOyT6JeVy1R",
        "outputId": "b6ccfe85-572b-4c9c-9a40-b8325ce64a4e"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3, 4, 5, 6, 3, 4, 5, 6, 7, 8, 9]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1=sc.parallelize([1,2,3,4,5,6])\n",
        "\n",
        "rdd2=sc.parallelize([3,4,5,6,7,8,9])\n",
        "\n",
        "## Intersection==> it gives ud only cpmmon values.\n",
        "\n",
        "rdd3=rdd1.intersection(rdd2)\n",
        "\n",
        "print(rdd3.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPlTxt9BVyyJ",
        "outputId": "67fd2aaa-2554-4b29-9007-5fb229594212"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4, 5, 6, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1=sc.parallelize([(1, \"Shubham\"),(2, \"Charan\"),(3, \"Bharath\")])\n",
        "\n",
        "rdd2=sc.parallelize([(1, \"Vishnu\"), (2, \"Charan\")])\n",
        "\n",
        "rdd3=rdd1.join(rdd2)\n",
        "### Join function==> this function give the common values on the basis of the Key.\n",
        "print(rdd3.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4RiSOswVyug",
        "outputId": "de8afcf2-b663-4bf3-d49d-abc4980e3e92"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(1, ('Shubham', 'Vishnu')), (2, ('Charan', 'Charan'))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1=sc.parallelize([(1, \"Shubham\"),(2, \"Charan\")])\n",
        "\n",
        "rdd2=sc.parallelize([(1, \"Vishnu\"), (2, \"Charan\")])"
      ],
      "metadata": {
        "id": "HKTTQ5jrVypg"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark=SparkSession\\\n",
        "          .builder\\\n",
        "          .config(\"spark.jars.packages\",\"org.apache.spark:spark-avro_2.13:4.0.1\")\\\n",
        "          .appName(\"prac\")\\\n",
        "          .getOrCreate()\n",
        "sc=spark.sparkContext\n",
        "## createDataFrame(Data,column[])\n",
        "\n",
        "df=spark.createDataFrame(rdd1,['id','name'])\n",
        "\n",
        "df.show() ## to get result of\n",
        "\n",
        "df.printSchema() ##  show the schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYn4WtD2hwpr",
        "outputId": "dee06777-d7db-48ce-93fe-33ca73ef55d8"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  1|Shubham|\n",
            "|  2| Charan|\n",
            "+---+-------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Aply our Schemas to our data set(manual way to apply schema)\n",
        "from pyarrow import StructScalar\n",
        "\n",
        "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
        "\n",
        "schema=StructType([\n",
        "    StructField(\"id\",IntegerType(),True),## True means we accepting the null values\n",
        "    StructField(\"name\",StringType(),True)\n",
        "])\n",
        "\n",
        "rdd1=sc.parallelize([(1, \"Shubham\"),(2, \"Charan\")])\n",
        "\n",
        "df=spark.createDataFrame(rdd1,schema = schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z72rC8mdhwkR",
        "outputId": "ba67149b-6f8c-4846-95e4-a6e3e6ebf2ae"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  1|Shubham|\n",
            "|  2| Charan|\n",
            "+---+-------+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "schema=StructType([\n",
        "    StructField(\"id\",IntegerType(),False),## False means we not accepting the null values\n",
        "    StructField(\"name\",StringType(),False)\n",
        "])\n",
        "\n",
        "rdd1=sc.parallelize([(1, \"Shubham\"),(2, \"Charan\")])\n",
        "\n",
        "df=spark.createDataFrame(rdd1,schema=schema)\n",
        "\n",
        "df.show()\n",
        "\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAc9tDY0hwdz",
        "outputId": "c5dddffe-a8fe-49c0-cbfa-483d5bc931af"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+\n",
            "| id|   name|\n",
            "+---+-------+\n",
            "|  1|Shubham|\n",
            "|  2| Charan|\n",
            "+---+-------+\n",
            "\n",
            "root\n",
            " |-- id: integer (nullable = false)\n",
            " |-- name: string (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## how to read and load diff file format in spark\n",
        "## 1. Cvs files\n",
        "\n",
        "csv_df=spark.read.csv(\"dept.csv\")\n",
        "\n",
        "csv_df.show() ## here we get data from our csv but column name are missplaced we got _c(column)\n",
        "\n",
        "csv_df.printSchema()"
      ],
      "metadata": {
        "id": "pte1WSt3hwak",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ae1edf4-63b5-4831-dce4-a253579cf710"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+\n",
            "|          _c0|       _c1|\n",
            "+-------------+----------+\n",
            "|accountNumber|assetValue|\n",
            "|        DEPT3|       500|\n",
            "|        DEPT3|       300|\n",
            "|        DEPT1|      1000|\n",
            "|        DEPT1|       700|\n",
            "|        DEPT1|       500|\n",
            "|        DEPT2|       400|\n",
            "|        DEPT2|       200|\n",
            "+-------------+----------+\n",
            "\n",
            "root\n",
            " |-- _c0: string (nullable = true)\n",
            " |-- _c1: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_df=spark.read.csv(\"dept.csv\",header=True) ## by using Header= True we get actul format with name of column\n",
        "\n",
        "csv_df.show()\n",
        "\n",
        "csv_df.printSchema() ## by default when we read a csv file the data type if default gets as a string type ."
      ],
      "metadata": {
        "id": "7Z1Awm-3FPCA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a98f5d05-dde9-4502-aaed-248024777b89"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+\n",
            "|accountNumber|assetValue|\n",
            "+-------------+----------+\n",
            "|        DEPT3|       500|\n",
            "|        DEPT3|       300|\n",
            "|        DEPT1|      1000|\n",
            "|        DEPT1|       700|\n",
            "|        DEPT1|       500|\n",
            "|        DEPT2|       400|\n",
            "|        DEPT2|       200|\n",
            "+-------------+----------+\n",
            "\n",
            "root\n",
            " |-- accountNumber: string (nullable = true)\n",
            " |-- assetValue: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## for accesing actual data type we have to use InferSchema = True to automnatically get the actual data type of our data.\n",
        "schema=StructType([\n",
        "    StructField(\"accountNumber\",StringType(),True),\n",
        "    StructField(\"assetValue\",IntegerType(),True)\n",
        "])\n",
        "csv_df=spark.read.csv(\"dept.csv\",header=True, schema=schema)\n",
        "\n",
        "csv_df.show()\n",
        "\n",
        "csv_df.printSchema()\n",
        "\n",
        "## Inferschema is usefull in only for Small data type in huge data it not work\n",
        "## that why wem have to sppy schema separately to call ."
      ],
      "metadata": {
        "id": "FGaBhyU9FO_j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06b4dda8-fdf5-41f0-80d5-3d1ebe43e6cc"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+\n",
            "|accountNumber|assetValue|\n",
            "+-------------+----------+\n",
            "|        DEPT3|       500|\n",
            "|        DEPT3|       300|\n",
            "|        DEPT1|      1000|\n",
            "|        DEPT1|       700|\n",
            "|        DEPT1|       500|\n",
            "|        DEPT2|       400|\n",
            "|        DEPT2|       200|\n",
            "+-------------+----------+\n",
            "\n",
            "root\n",
            " |-- accountNumber: string (nullable = true)\n",
            " |-- assetValue: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 2. Perquet files\n",
        "par_df=spark.read.parquet(\"data.parquet\") ## just accesing the data it takes default data type as Strring\n",
        "\n",
        "\n",
        "par_df.show()\n",
        "\n",
        "par_df.printSchema()"
      ],
      "metadata": {
        "id": "W4Ao6aQyFO5y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53f4aacb-8cce-4214-8449-ab391a1c601e"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+--------+----------------+\n",
            "|      id|     tdate|category|         product|\n",
            "+--------+----------+--------+----------------+\n",
            "|00000000|06-26-2011|Exercise|Gymnastics Rings|\n",
            "|00000002|06-01-2011|Exercise|Gymnastics Rings|\n",
            "+--------+----------+--------+----------------+\n",
            "\n",
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- tdate: string (nullable = true)\n",
            " |-- category: string (nullable = true)\n",
            " |-- product: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 3. Orc file\n",
        "\n",
        "orc_df=spark.read.orc(\"data.orc\")\n",
        "\n",
        "orc_df.show()\n",
        "\n",
        "orc_df.printSchema() ## default string type"
      ],
      "metadata": {
        "id": "dmoDUvCOFO1q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2a34df2-84e9-4389-99a3-fb20c2ae232c"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+--------------------+------------------+-----------+----------+-----+-----+---+------------+------------+--------------------+--------------------+\n",
            "|first_name|last_name|        company_name|           address|       city|    county|state|  zip|age|      phone1|      phone2|               email|                 web|\n",
            "+----------+---------+--------------------+------------------+-----------+----------+-----+-----+---+------------+------------+--------------------+--------------------+\n",
            "|   Solange|   Shinko|   Mosocco, Ronald A|       426 Wolf St|   Metairie| Jefferson|   LA|70002| 21|504-979-9175|504-265-8174|  solange@shinko.com|http://www.mosocc...|\n",
            "|    Arlene|  Klusman|Beck Horizon Buil...|        3 Secor Rd|New Orleans|   Orleans|   LA|70112| 20|504-710-5840|504-946-1807|arlene_klusman@gm...|http://www.beckho...|\n",
            "|     Larae|   Gudroe|Lehigh Furn Divsn...| 6651 Municipal Rd|      Houma|Terrebonne|   LA|70360| 33|985-890-7262|985-261-5783|larae_gudroe@gmai...|http://www.lehigh...|\n",
            "| Willodean|Konopacki|            Magnuson| 55 Hawthorne Blvd|  Lafayette| Lafayette|   LA|70506| 22|337-253-8384|337-774-7564|willodean_konopac...|http://www.magnus...|\n",
            "|  Terrilyn|Rodeigues|      Stuart J Agins|    3718 S Main St|New Orleans|   Orleans|   LA|70130| 33|504-463-4384|504-635-8518|terrilyn.rodeigue...|http://www.stuart...|\n",
            "|  Kayleigh|     Lace|Dentalaw Divsn Hl...|43 Huey P Long Ave|  Lafayette| Lafayette|   LA|70508| 11|337-740-9323|337-751-2326|kayleigh.lace@yah...|http://www.dental...|\n",
            "|     Jutta|    Amyot|National Medical ...|      49 N Mays St|  Broussard| Lafayette|   LA|70518| 15|337-515-1438|337-991-8070|  jamyot@hotmail.com|http://www.nation...|\n",
            "|  Cordelia| Storment|  Burrows, Jon H Esq|    393 Hammond Dr|  Lafayette| Lafayette|   LA|70506| 16|337-566-6001|337-255-3427|cordelia_storment...|http://www.burrow...|\n",
            "+----------+---------+--------------------+------------------+-----------+----------+-----+-----+---+------------+------------+--------------------+--------------------+\n",
            "\n",
            "root\n",
            " |-- first_name: string (nullable = true)\n",
            " |-- last_name: string (nullable = true)\n",
            " |-- company_name: string (nullable = true)\n",
            " |-- address: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- county: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- zip: string (nullable = true)\n",
            " |-- age: string (nullable = true)\n",
            " |-- phone1: string (nullable = true)\n",
            " |-- phone2: string (nullable = true)\n",
            " |-- email: string (nullable = true)\n",
            " |-- web: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 4 avro file\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark=SparkSession\\\n",
        "          .builder\\\n",
        "          .config(\"spark.jars.packages\",\"org.apache.spark:spark-avro_2.11:4.0.0\")\\\n",
        "          .appName(\"prac\")\\\n",
        "          .getOrCreate()\n",
        "avro_df=spark.read.format(\"avro\").load(\"data.avro\")\n",
        "\n",
        "avro_df.show()\n",
        "\n",
        "avro_df.printSchema()\n",
        "\n",
        "## hre all sytax is correct but by colab we get error\n",
        "\n",
        "## Maven Reposetory:to get sytaxt to call avro file\n"
      ],
      "metadata": {
        "id": "G_vPYyJtFOwJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "e2db8780-73da-48f8-aed4-510fdd6e4f9f"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of Apache Avro Data Source Guide.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3897612017.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prac\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mavro_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"avro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data.avro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mavro_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1362\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1363\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Failed to find data source: avro. Avro is built-in but external data source module since Spark 2.4. Please deploy the application as per the deployment section of Apache Avro Data Source Guide."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## to get our spark version\n",
        "print(spark.version)"
      ],
      "metadata": {
        "id": "zgVvHPacFOt0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29fbdf9e-6c3b-4fe1-8e7a-b106214bf641"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 5.json file\n",
        "json_df=spark.read.json(\"pets1.json\")\n",
        "\n",
        "json_df.show()\n",
        "\n",
        "json_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qxVGBXajQXf",
        "outputId": "1a47ccd2-a1a0-46b0-9cb4-09b73406919b"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+----+----------+\n",
            "|Boolean|  Mobile|Name|      Pets|\n",
            "+-------+--------+----+----------+\n",
            "|   true|12345678|Test|[Dog, cat]|\n",
            "+-------+--------+----+----------+\n",
            "\n",
            "root\n",
            " |-- Boolean: boolean (nullable = true)\n",
            " |-- Mobile: long (nullable = true)\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Pets: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GqwOq-xIjQT6"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ojz15ScVjQRL"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Vx2N-pcjQOL"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8AFP2EF6jQLL"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aHk847PXjQIT"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yp85ti8kjQFE"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xBjFUF3DjP_a"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T00kbpfqjP9A"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sz6OpOTvjP54"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mNuD7o3yjP3e"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1S0DPPGZjP0D"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ufh9pg0WjPw9"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PNei4iGvjPrd"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0vE_ez8_jPn1"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IM0DbawHjPDk"
      },
      "execution_count": 101,
      "outputs": []
    }
  ]
}