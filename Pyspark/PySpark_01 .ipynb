{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8xDkJLho51W",
        "outputId": "6234f318-9f8f-4770-8b7e-7f220e069f7a"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pyspark'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkContext, SparkConf\n\u001b[32m      3\u001b[39m conf=SparkConf().setAppName(\u001b[33m\"\u001b[39m\u001b[33mSample\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m sc=SparkContext(conf=conf)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pyspark'"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "conf=SparkConf().setAppName(\"Sample\")\n",
        "\n",
        "sc=SparkContext(conf=conf)\n",
        "\n",
        "list1=[1,2,3,4,5,6,7,8,9]\n",
        "print(type(list1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xd41npXLpVqb",
        "outputId": "6ad302ba-34f8-4330-e01a-9888148a6d97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pyspark.core.rdd.RDD'>\n"
          ]
        }
      ],
      "source": [
        "## we have to convert this list in RDDs\n",
        "rdd1=sc.parallelize(list1)\n",
        "print(type(rdd1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YKipkB-pVmf"
      },
      "outputs": [],
      "source": [
        "## every type of data set is able to convert in RDDs in Spark\n",
        "## We convert data into RDDs and DF(DataFrames) To process on the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROIIznlLpVeo",
        "outputId": "26215c19-7baf-4f34-c0e3-278bf7401d6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 6]\n"
          ]
        }
      ],
      "source": [
        "## Task1: result rdd should contain <=6\n",
        "\n",
        "##Here we are using the Filter Function To Filter out our elements or value\n",
        "# Filter Function\n",
        "\n",
        "rdd2=rdd1.filter(lambda x: x<=6)\n",
        "\n",
        "print(rdd2.collect())\n",
        "\n",
        "## .collect() is use to print out the our output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvpX9F5kpVb2",
        "outputId": "4a94422c-bdd9-40a0-d360-fb58696c6102"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[16, 17, 18, 19, 20, 21, 22, 23, 24]\n"
          ]
        }
      ],
      "source": [
        "## Task 2: here we wnant to increse value of every element by 15 ..\n",
        "\n",
        "## map Function= Bye using this function we can do aply on the element to what we want\n",
        "\n",
        "add_rdd=rdd1.map(lambda x: x+15)\n",
        "\n",
        "print(add_rdd.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6zx-9MTsNHu"
      },
      "outputs": [],
      "source": [
        "#Home Work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkIypf7FsNFB"
      },
      "outputs": [],
      "source": [
        "## Explain Spark Architecture and its Components\n",
        "# Spark follows a masterâ€“slave architecture with a Driver Program coordinating tasks\n",
        "#  and Cluster Managers allocating resources to Executors across worker nodes.\n",
        "#   Its main components are the Driver, Cluster Manager, Worker Nodes, Executors,\n",
        "#   and the RDD/DataFrame API that enable distributed computation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMu6xV6jvX5Q"
      },
      "source": [
        "Core Components\n",
        "- ** Driver Program\n",
        "- Runs the main application.\n",
        "- Converts user code into tasks and schedules them.\n",
        "- Maintains the SparkContext, which is the entry point to Spark.\n",
        "\n",
        "- **Cluster Manager\n",
        "- Allocates resources across the cluster.\n",
        "- Examples: Standalone, YARN, Mesos, Kubernetes.\n",
        "- Decides how many executors and cores each application gets.\n",
        "\n",
        "- ** Worker Nodes\n",
        "- Machines in the cluster that run tasks.\n",
        "- Each worker hosts executors.\n",
        "- ** Executors\n",
        "- JVM processes launched on worker nodes.\n",
        "- Execute tasks assigned by the driver.\n",
        "- Store data in memory for caching and reuse.\n",
        "- ** Tasks\n",
        "- Smallest unit of work.\n",
        "- Sent by the driver to executors for execution.\n",
        "\n",
        "- ** RDDs (Resilient Distributed Datasets)\n",
        "- Fundamental data structure in Spark.\n",
        "- Immutable, distributed collections of objects.\n",
        "- Support transformations (map, filter) and actions (collect, count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WVkEXQCsNDJ"
      },
      "outputs": [],
      "source": [
        "## Diff.Between Haddop And Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuRZU13asNAI",
        "outputId": "28e58257-af47-45b7-c82b-117a44f52f11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['Hallo', 'Students'], ['Pyspark', 'session', 'is', 'started']]\n"
          ]
        }
      ],
      "source": [
        "list1=[\"Hallo Students\",\"Pyspark session is started\"]\n",
        "list_rdd=sc.parallelize(list1)\n",
        "## Split the list basis on the space\n",
        "## Split Function: Split the list bsis on what we mention(lambd X: x.split(' '))\n",
        "\n",
        "list_rdd=list_rdd.map(lambda  x: x.split(\" \"))\n",
        "\n",
        "print(list_rdd.collect())\n",
        "\n",
        "## Here split function seperated evry word on basis of space but every string converted in seperate list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rqkrvNOpVY2",
        "outputId": "bbc42aa0-3c26-4898-ae58-a56975eb42c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hallo', 'Students', 'Pyspark', 'session', 'is', 'started']\n"
          ]
        }
      ],
      "source": [
        "list1=[\"Hallo Students\",\"Pyspark session is started\"]\n",
        "list_rdd=sc.parallelize(list1)\n",
        "## flatmap Split the list basis on the space\n",
        "## Flatmap Function: Split the list bsis on what we mention(lambd X: x.split(' '))\n",
        "\n",
        "list_rdd=list_rdd.flatMap(lambda  x: x.split(\" \"))\n",
        "\n",
        "print(list_rdd.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zrdt0airdx0d",
        "outputId": "3513ae6a-c54b-435d-ed95-bdaaa2df1739"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[5, 8, 7, 7, 2, 7]\n"
          ]
        }
      ],
      "source": [
        "list1=[\"Hallo Students\",\"Pyspark session is started\"]\n",
        "list_rdd=sc.parallelize(list1)\n",
        "\n",
        "list_rdd=list_rdd.flatMap(lambda  x: x.split(\" \"))\n",
        "\n",
        "\n",
        "rdd1=list_rdd.map(lambda x: len(x))\n",
        "print(rdd1.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdRmbdQbdxw3",
        "outputId": "f2725d83-6716-4f74-e496-3ebfe7b89a7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hallo', 'students', 'pyspark', 'session', 'is', 'started', 'students', 'are', 'doing', 'pyspark', 'scenarios']\n",
            "[('Hallo', 1), ('students', 1), ('pyspark', 1), ('session', 1), ('is', 1), ('started', 1), ('students', 1), ('are', 1), ('doing', 1), ('pyspark', 1), ('scenarios', 1)]\n",
            "[('students', 2), ('session', 1), ('started', 1), ('are', 1), ('doing', 1), ('Hallo', 1), ('pyspark', 2), ('is', 1), ('scenarios', 1)]\n"
          ]
        }
      ],
      "source": [
        "list1=[\"Hallo students\",\"pyspark session is started\",\"students are doing pyspark scenarios\"]\n",
        "list_rdd=sc.parallelize(list1)\n",
        "\n",
        "## we want the count of each word pressented\n",
        "flat_rdd=list_rdd.flatMap(lambda x: x.split(\" \"))\n",
        "\n",
        "print(flat_rdd.collect())\n",
        "\n",
        "count_rdd=flat_rdd.map(lambda x: (x,1))\n",
        "print(count_rdd.collect())\n",
        "\n",
        "word_count= count_rdd.reduceByKey(lambda x, b: x+b)\n",
        "print(word_count.collect())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fk06FO3wdxt7",
        "outputId": "5f5971b5-8dda-4df7-c3f1-1069e4467a94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 3, 2, 4, 3, 2, 3, 4, 5, 5, 5, 4, 3, 2, 3, 2, 3, 4, 6]\n",
            "[2, 4, 6, 1, 3, 5]\n"
          ]
        }
      ],
      "source": [
        "## Distinct function: Removing duplicates from the rdd\n",
        "\n",
        "list1=[1,2,3,4,3,2,4,3,2,3,4,5,5,5,4,3,2,3,2,3,4,6]\n",
        "\n",
        "list1_rdd=sc.parallelize(list1)\n",
        "print(list1_rdd.collect())\n",
        "\n",
        "distinct_rdd=list1_rdd.distinct()\n",
        "print(distinct_rdd.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z4jpYRUdxiu",
        "outputId": "f0d212d7-2c4e-4d6f-f032-d323a846bb9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello Shubham', 'hello Vishnu ', 'Hello Charan', 'Hello Bharath']\n"
          ]
        }
      ],
      "source": [
        "  ## Read the txt file and create RDDs\n",
        "txt_rdd=sc.textFile('Hello Shubham.txt')\n",
        "print(txt_rdd.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvfndzTmdxfU",
        "outputId": "7c314253-606f-458d-8140-42045c576fba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', 'Shubham', 'hello', 'Vishnu', '', 'Hello', 'Charan', 'Hello', 'Bharath']\n",
            "[('Hello', 1), ('Shubham', 1), ('hello', 1), ('Vishnu', 1), ('', 1), ('Hello', 1), ('Charan', 1), ('Hello', 1), ('Bharath', 1)]\n",
            "[('Shubham', 1), ('hello', 1), ('', 1), ('Charan', 1), ('Bharath', 1), ('Hello', 3), ('Vishnu', 1)]\n"
          ]
        }
      ],
      "source": [
        "## now i want get count of each word\n",
        "rdd1=sc.textFile('Hello Shubham.txt')\n",
        "\n",
        "flat_rdd1=rdd1.flatMap(lambda x: x.split(\" \"))\n",
        "print(flat_rdd1.collect())\n",
        "\n",
        "count_rdd1=flat_rdd1.map(lambda x: (x,1))\n",
        "print(count_rdd1.collect())\n",
        "\n",
        "word_count=count_rdd1.reduceByKey(lambda x,y: x+y)\n",
        "print(word_count.collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ua2PQp8wdxcg",
        "outputId": "46cd9e27-e1e1-445e-8a1e-afefaf33b34c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', 'Shubham', 'hello', 'Vishnu', '', 'Hello', 'Charan', 'Hello', 'Bharath']\n",
            "[('Hello', 1), ('Shubham', 1), ('hello', 1), ('Vishnu', 1), ('', 1), ('Hello', 1), ('Charan', 1), ('Hello', 1), ('Bharath', 1)]\n",
            "[('Shubham', 1), ('hello', 1), ('', 1), ('Charan', 1), ('Bharath', 1), ('Hello', 3), ('Vishnu', 1)]\n"
          ]
        }
      ],
      "source": [
        "rdd1=sc.textFile('Hello Shubham.txt')\n",
        "\n",
        "flat_rdd1=rdd1.flatMap(lambda x: x.split(\" \"))\n",
        "print(flat_rdd1.collect())\n",
        "\n",
        "count_rdd1=flat_rdd1.map(lambda x: (x,1))\n",
        "print(count_rdd1.collect())\n",
        "\n",
        "word_count=count_rdd1.reduceByKey(lambda x,y: x+y)\n",
        "print(word_count.collect())\n",
        "\n",
        "\n",
        "##now we want to save this changes as text file\n",
        "## here we use the Function called saveAsTextFile\n",
        "word_count.saveAsTextFile(\"Word Count Text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNE9MHTwdxZm",
        "outputId": "ed34b345-0e99-48c5-bc5b-ae09140ca128"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 6, 3, 5, 6, 3, 4, 5, 3, 234, 5, 35, 2, 3, 5]\n",
            "[1, 2, 3, 4, 5, 6, 3]\n"
          ]
        }
      ],
      "source": [
        "## take function = we can mention hoy many elements we have to show by using take function\n",
        "\n",
        "numbers=[1,2,3,4,5,6,3,5,6,3,4,5,3,234,5,35,2,3,5,]\n",
        "numbers_rdd=sc.parallelize(numbers)\n",
        "print(numbers_rdd.collect())\n",
        "\n",
        "\n",
        "print(numbers_rdd.take(7))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz5H9lY92B3m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-gk2qLK2B00"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Zaz62Pe2Byw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cgr-Kd9h2BwI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIBdQCTr2BtN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIqsDHP12Bqe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNii7-vU2BnF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2MCyrS5dxWf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahKgJ-BBdxTV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlnDSdznpVRI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2CVLuGupVOg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
